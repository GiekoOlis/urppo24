{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Ekaterina Ugolnikova giekoolis@gmail.com\n",
    "13.10.2022\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook to process V&F Dataset to necessary view\n",
    "\n",
    "Originally dataset looks like:\n",
    "```V&F_Dataset:.\n",
    "│   table_of_correspondences.xlsx\n",
    "│   \n",
    "└───originals\n",
    "    ├───001\n",
    "    │   ├───Depersonification_data\n",
    "    │   │       001_FD.dcm\n",
    "    │   │       001_SD.dcm\n",
    "    │   │       \n",
    "    │   ├───Labeled_data\n",
    "    │   │   ├───001_FD\n",
    "    │   │   │       C1.mrk.json\n",
    "    │   │   │       ...\n",
    "    │   │   │       FH2.mrk.json\n",
    "    │   │   │ \n",
    "    │   │   └───001_SD\n",
    "    │   │           C1.mrk.json\n",
    "    │   │           ...\n",
    "    │   │           FH2.mrk.json\n",
    "    │   │               \n",
    "    │   └───Primary_Data\n",
    "    │           1.2.840.113619.2.203.4.2147483647.1644922548.475876.dcm\n",
    "    │           ...\n",
    "    │           1.2.840.113619.2.203.4.2147483647.1644922794.309142.dcm\n",
    "    │           \n",
    "    ├───002\n",
    "    ├───...\n",
    "    └───099\n",
    "```\n",
    "\n",
    "The view we need for further work:\n",
    "\n",
    "```V&F_Dataset:.\n",
    "├───originals\n",
    "│   ├───001\n",
    "│   │       001_FD.dcm\n",
    "│   │       001_SD.dcm\n",
    "│   │        \n",
    "│   ├───002   \n",
    "│   ├───...    \n",
    "│   └───099\n",
    "│\n",
    "└───labels\n",
    "    ├───001\n",
    "    │   ├───001_FD\n",
    "    │   │       C1.mrk.json\n",
    "    │   │       ...\n",
    "    │   │       FH2.mrk.json\n",
    "    │   │   \n",
    "    │   └───001_SD\n",
    "    │           C1.mrk.json\n",
    "    │           ...\n",
    "    │           FH2.mrk.json\n",
    "    │           \n",
    "    ├───002\n",
    "    ├───...\n",
    "    └───099\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check original dataset for correctness\n",
    "\n",
    "Firstly, we will check if the original dataset corresponds to its described view.\n",
    "\n",
    "Run chunk below to check.\n",
    "\n",
    "If theres messages like ```\"Folder [ *** ] skipped bcs ...\"``` please go through each message and check these folders for typos in names or missing data\n",
    "\n",
    "After you've done correcting dataset please run this chunk again to check if theres no more errors.\n",
    "\n",
    "If the output of chunk is a message ```\"Everything is fine\"``` you can process to the next part of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk\n"
     ]
    }
   ],
   "source": [
    "print(\"kk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathlib\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pathlib\n",
      "Successfully installed pathlib-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\gieko\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gieko\\Dropbox\\NIITO_Vertebrae\\NIITO_Vertebrae_Dataset\\NIITO_Vertebrae_Dataset_Test_original\n",
      "Everything is fine\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "all_labels = ['C2.mrk.json', 'C3.mrk.json', 'C4.mrk.json', 'C5.mrk.json', 'C6.mrk.json', 'C7.mrk.json', 'FH1.mrk.json', 'FH2.mrk.json', 'L1.mrk.json', 'L2.mrk.json', 'L3.mrk.json', 'L4.mrk.json', 'L5.mrk.json', 'S1.mrk.json', 'Th1.mrk.json', 'Th10.mrk.json', 'Th11.mrk.json', 'Th12.mrk.json', 'Th2.mrk.json', 'Th3.mrk.json', 'Th4.mrk.json', 'Th5.mrk.json', 'Th6.mrk.json', 'Th7.mrk.json', 'Th8.mrk.json', 'Th9.mrk.json']\n",
    "\n",
    "# path_to_main_catalog = \"C:\\\\Users\\\\EUgolnikova\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\"\n",
    "# path_to_main_catalog = \"C:\\\\Users\\\\EUgolnikova\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\"\n",
    "path_to_main_catalog = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\"\n",
    "# path_to_main_catalog = \"C:\\\\Users\\\\EUgolnikova\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset6-10\"\n",
    "# path_to_main_catalog = \"C:\\\\Users\\\\EUgolnikova\\\\Desktop\"\n",
    "\n",
    "# dataset_name = \"NIITO_Vertebrae_Dataset\"\n",
    "# dataset_name = \"на отправку\"\n",
    "dataset_name = \"NIITO_Vertebrae_Dataset_Test_original\"\n",
    "\n",
    "\n",
    "try:\n",
    "    absolute_path = Path(path_to_main_catalog).resolve(strict=True)\n",
    "\n",
    "except FileNotFoundError as fnf_error:\n",
    "    print(fnf_error)\n",
    "else:\n",
    "    path_to_originals = os.path.join(path_to_main_catalog, dataset_name)\n",
    "    print(path_to_originals)\n",
    "    folders = os.listdir(path_to_originals)\n",
    "    skipped_folders = []\n",
    "    for folder in folders:\n",
    "        folders_in_case = os.listdir(os.path.join(path_to_originals, folder))\n",
    "        try:\n",
    "            path_to_labels = os.path.join(path_to_originals, *[folder, \"Labeled_data\"])\n",
    "            path_to_anonyms = os.path.join(path_to_originals, *[folder, \"Depersonification_data\"])\n",
    "            labeled_path = Path(path_to_anonyms).resolve(strict=True)\n",
    "            anonym_path = Path(path_to_labels).resolve(strict=True)\n",
    "        except FileNotFoundError as fnf_error:\n",
    "            print(\"Folder [\", folder, \"] skipped bcs theres no 'Labeled_data' or 'Depersonification_data'\")\n",
    "            skipped_folders.append(folder)\n",
    "        else:\n",
    "            dimentions = os.listdir(path_to_labels)\n",
    "            files = os.listdir(path_to_anonyms)\n",
    "            try:\n",
    "                path_to_markdown_sd = os.path.join(path_to_labels, folder + \"_SD\")\n",
    "                q = Path(path_to_markdown_sd).resolve(strict=True)\n",
    "                files = os.listdir(path_to_markdown_sd)\n",
    "                if len(files) == len(all_labels):\n",
    "                    if set(files).issubset(all_labels):\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(\"Folder [\", folder, \"] skipped bcs theres wrong names of files in ***_SD folder\")\n",
    "                        skipped_folders.append(folder)\n",
    "                else:\n",
    "                    print(\"Folder [\", folder, \"] skipped bcs theres wrong number of files in ***_SD folder\")\n",
    "                    skipped_folders.append(folder)\n",
    "                    \n",
    "            \n",
    "            except:\n",
    "                print(\"Folder [\", folder, \"] skipped bcs theres no ***_SD folder\")\n",
    "                skipped_folders.append(folder)\n",
    "            \n",
    "            try:\n",
    "                path_to_anonym_sd = os.path.join(path_to_anonyms, folder + \"_SD.dcm\")\n",
    "                path_to_anonym_fd = os.path.join(path_to_anonyms, folder + \"_SD.dcm\")\n",
    "                sd = Path(path_to_anonym_sd).resolve(strict=True)\n",
    "                fd = Path(path_to_anonym_fd).resolve(strict=True)\n",
    "            except:\n",
    "                print(\"Folder [\", folder, \"] skipped bcs theres no ***_SD.dcm or ***_FD.dcm file\")\n",
    "                skipped_folders.append(folder)\n",
    "\n",
    "if len(skipped_folders) == 0:\n",
    "    print(\"Everything is fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform dataset to necessary view\n",
    "\n",
    "Run chunk below when you sure the original dataset corresponds to its described view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gieko\\Dropbox\\NIITO_Vertebrae\\NIITO_Vertebrae_Dataset\\NIITO_Vertebrae_Dataset_Test_original\n"
     ]
    }
   ],
   "source": [
    "print(path_to_originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# transformed_dataset = os.path.join(path_to_main_catalog, \"NIITO_Vertebrae_Dataset\")\n",
    "transformed_dataset = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Test\"\n",
    "\n",
    "if not os.path.exists(transformed_dataset):\n",
    "    os.makedirs(transformed_dataset)\n",
    "\n",
    "transformed_originals = os.path.join(transformed_dataset, \"images\")\n",
    "transformed_labels = os.path.join(transformed_dataset, \"labels\")\n",
    "if not os.path.exists(transformed_originals):\n",
    "\n",
    "    os.makedirs(transformed_originals)\n",
    "if not os.path.exists(transformed_labels):\n",
    "    os.makedirs(transformed_labels)\n",
    "\n",
    "folders = os.listdir(path_to_originals)\n",
    "for folder in folders:\n",
    "\n",
    "    current_folder_orig = os.path.join(path_to_originals, folder)\n",
    "\n",
    "    trans_orig = os.path.join(transformed_originals, folder)\n",
    "    if not os.path.exists(trans_orig):\n",
    "        os.mkdir(trans_orig)\n",
    "        \n",
    "    trans_labs = os.path.join(transformed_labels, folder)\n",
    "    if not os.path.exists(trans_labs):\n",
    "        os.mkdir(trans_labs)\n",
    "\n",
    "    trans_labs_fd = os.path.join(trans_labs, folder + \"_FD\")\n",
    "    if not os.path.exists(trans_labs_fd):\n",
    "        os.mkdir(trans_labs_fd)\n",
    "\n",
    "    trans_labs_sd = os.path.join(trans_labs, folder + \"_SD\")\n",
    "    if not os.path.exists(trans_labs_sd):\n",
    "        os.mkdir(trans_labs_sd)\n",
    "\n",
    "    \n",
    "    orig_anon = os.path.join(current_folder_orig, \"Depersonification_data\")\n",
    "    orig_labs = os.path.join(current_folder_orig, \"Labeled_data\")\n",
    "    orig_labs_fd = os.path.join(orig_labs, folder + \"_FD\")\n",
    "    orig_labs_sd = os.path.join(orig_labs, folder + \"_SD\")\n",
    "\n",
    "    for anon in os.listdir(orig_anon):\n",
    "        shutil.copy2(os.path.join(orig_anon, anon), trans_orig)\n",
    "    \n",
    "    for markdown in os.listdir(orig_labs_sd):\n",
    "        shutil.copy2(os.path.join(orig_labs_sd, markdown), trans_labs_sd)\n",
    "\n",
    "    # for markdown in os.listdir(orig_labs_fd):\n",
    "    #     shutil.copy2(os.path.join(orig_labs_fd, markdown), trans_labs_fd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53c81479bf08f4a80b6cd28867cbdb53cea6802fa2989097f9e695215961dbef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
